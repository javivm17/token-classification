{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maribelrb/opt/anaconda3/envs/PPIBot/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from firebase import firebase\n",
    "import json\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, load_metric\n",
    "import numpy as np\n",
    "import re\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura e integraciÃ³n de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/parser_training_data.json', 'r') as f:\n",
    "    training_data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de crowdsourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "FIREBASE_URL = os.getenv(\"FIREBASE_URL\")    \n",
    "\n",
    "firebase_url = firebase.FirebaseApplication(FIREBASE_URL, None)\n",
    "\n",
    "best_paraphrases_firebase = firebase_url.get('/best_paraphrases', None)\n",
    "firebase_id = list(best_paraphrases_firebase)[0]\n",
    "best_paraphrases = best_paraphrases_firebase[firebase_id]\n",
    "\n",
    "paraphrases = firebase_url.get('/paraphrases', None)\n",
    "firebase_paraphrase_id = list(paraphrases)[0]\n",
    "tagged_paraphrases = paraphrases[firebase_paraphrase_id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos generados con chatito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/training_chatito.json', 'r') as f:\n",
    "    training_chatito = json.load(f)[\"measures\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_annotation(paraphrase, tagged_paraphrases):\n",
    "    same_paraphrases = [p[\"annotation\"] for p in tagged_paraphrases if p[\"description\"] == paraphrase]\n",
    "    number_occurrences = [same_paraphrases.count(a) for a in same_paraphrases]\n",
    "    max_occurrences = max(number_occurrences)\n",
    "    return same_paraphrases[number_occurrences.index(max_occurrences)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IntegraciÃ³n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is necessary to remove duplicates from the paraphrases\n",
    "paraphrases = set([phrase[\"description\"] for phrase in tagged_paraphrases[\"data\"]])\n",
    "for paraphrase in paraphrases:\n",
    "    annotation = get_best_annotation(paraphrase, tagged_paraphrases[\"data\"])\n",
    "    best_paraphrases[\"data\"].append({\"description\": paraphrase, \"annotation\": annotation})\n",
    "\n",
    "best_paraphrases[\"data\"].extend(training_data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_chatito_parsed = []\n",
    "for phrase in training_chatito:\n",
    "    description = \"\".join([value[\"value\"] for value in phrase])\n",
    "    slots = [{'text': value[\"value\"].strip(), 'tag': 'O'} if \"slot\" not in value else {'text': value[\"value\"].strip(), 'tag': value[\"slot\"]} for value in phrase]\n",
    "    slots_cleaned = [slot for slot in slots if slot[\"text\"] != \"\"]\n",
    "    slots_list = []\n",
    "    for slot in slots_cleaned:\n",
    "        slots_list.append({'text': slot[\"text\"], 'tag': slot[\"tag\"]})\n",
    "    training_chatito_parsed.append({\"description\": description, \"annotation\": slots_list})\n",
    "\n",
    "best_paraphrases[\"data\"].extend(training_chatito_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases: 2190\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of phrases: {len(best_paraphrases[\"data\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_TAGS = [\"TMI\", \"TSI\", \"TSE\", \"TEI\", \"TEE\", \"TBE\"]\n",
    "COUNT_TAGS = [\"CMI\", \"CE\"]\n",
    "DATA_TAGS = [\"AttributeName, AttributeValue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time phrases: 2088\n",
      "Number of count phrases: 81\n",
      "Number of data phrases: 0\n"
     ]
    }
   ],
   "source": [
    "time_phrases = []\n",
    "count_phrases = []\n",
    "data_phrases = []\n",
    "\n",
    "for phrase in best_paraphrases[\"data\"]:\n",
    "    text = phrase[\"description\"]\n",
    "    labels = set([label[\"tag\"] for label in phrase[\"annotation\"]])\n",
    "    if len(labels.intersection(TIME_TAGS)) > 0:\n",
    "        time_phrases.append(phrase)\n",
    "    elif len(labels.intersection(COUNT_TAGS)) > 0:\n",
    "        count_phrases.append(phrase)\n",
    "    elif len(labels.intersection(DATA_TAGS)) > 0:\n",
    "        data_phrases.append(phrase)\n",
    "\n",
    "print(f'Number of time phrases: {len(time_phrases)}')\n",
    "print(f'Number of count phrases: {len(count_phrases)}')\n",
    "print(f'Number of data phrases: {len(data_phrases)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "useless_tags = [\"TMI\", \"TSI\", \"TEI\", \"GBI\"]\n",
    "\n",
    "for phrase in time_phrases:\n",
    "    annotations = []\n",
    "    for slot in phrase[\"annotation\"]:\n",
    "        slot_object = {}\n",
    "        slot_object[\"value\"] = slot[\"text\"]\n",
    "        slot_object[\"type\"] = \"Slot\"\n",
    "        slot_object[\"slot\"] = slot[\"tag\"] if slot[\"tag\"] not in useless_tags else \"O\"\n",
    "        annotations.append(slot_object)\n",
    "\n",
    "    data.append(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tags = []\n",
    "\n",
    "for phrase in data:\n",
    "    phrase_tokens = []\n",
    "    phrase_tags = []\n",
    "    for slot in phrase:\n",
    "        splits = slot[\"value\"].split(\" \")\n",
    "        tag = slot[\"slot\"]\n",
    "        for i in range(len(splits)):\n",
    "            if tag != \"O\":\n",
    "                if i == 0:\n",
    "                    phrase_tokens.append(splits[i])\n",
    "                    phrase_tags.append(\"B-\"+tag)\n",
    "                else:\n",
    "                    phrase_tokens.append(splits[i])\n",
    "                    phrase_tags.append(\"I-\"+tag)\n",
    "            else:\n",
    "                phrase_tokens.append(splits[i])\n",
    "                phrase_tags.append(tag)\n",
    "    tokens.append(phrase_tokens)\n",
    "    tags.append(phrase_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-TSE', 'B-AGR', 'B-CCI', 'I-TEE', 'I-AGR', 'I-AttributeValue', 'O', 'B-TEE', 'B-TBE', 'B-AttributeValue', 'B-GBC', 'B-TSE', 'I-TBE', 'I-GBC', 'I-CCI']\n"
     ]
    }
   ],
   "source": [
    "tags_list = list(set([tag for phrase in tags for tag in phrase]))\n",
    "print(tags_list)\n",
    "\n",
    "labels = [[tags_list.index(label) for label in phrase] for phrase in tags]\n",
    "\n",
    "examples = {\n",
    "    \"tokens\": tokens,\n",
    "    \"tags\": labels\n",
    "}\n",
    "\n",
    "datasets = Dataset.from_dict(examples).train_test_split(test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.91ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.27ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"TimeClassification\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/47swnvh93cl0_bw1dw8tl_0h0000gn/T/ipykernel_51664/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [tags_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [tags_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1670\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 81\n",
      "  Number of trainable parameters = 108903183\n",
      "  0%|          | 0/81 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [05:26<09:19, 10.37s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 418\n",
      "  Batch size = 64\n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 27/81 [05:47<09:19, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1727917194366455, 'eval_precision': 0.30176355323318094, 'eval_recall': 0.30156657963446476, 'eval_f1': 0.30166503428011754, 'eval_accuracy': 0.66948109058927, 'eval_runtime': 20.9418, 'eval_samples_per_second': 19.96, 'eval_steps_per_second': 0.334, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 54/81 [10:44<03:28,  7.73s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 418\n",
      "  Batch size = 64\n",
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 54/81 [11:05<03:28,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5931911468505859, 'eval_precision': 0.5988603988603989, 'eval_recall': 0.6860313315926893, 'eval_f1': 0.6394888956495284, 'eval_accuracy': 0.840457343887423, 'eval_runtime': 21.6018, 'eval_samples_per_second': 19.35, 'eval_steps_per_second': 0.324, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [26:46<00:00,  6.51s/it] The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 418\n",
      "  Batch size = 64\n",
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [27:04<00:00,  6.51s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [27:04<00:00, 20.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44541996717453003, 'eval_precision': 0.6823461091753774, 'eval_recall': 0.7669712793733682, 'eval_f1': 0.7221880762138906, 'eval_accuracy': 0.8824978012313105, 'eval_runtime': 17.1061, 'eval_samples_per_second': 24.436, 'eval_steps_per_second': 0.409, 'epoch': 3.0}\n",
      "{'train_runtime': 1624.108, 'train_samples_per_second': 3.085, 'train_steps_per_second': 0.05, 'train_loss': 1.1433769508644387, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=81, training_loss=1.1433769508644387, metrics={'train_runtime': 1624.108, 'train_samples_per_second': 3.085, 'train_steps_per_second': 0.05, 'train_loss': 1.1433769508644387, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 418\n",
      "  Batch size = 64\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:12<00:00,  1.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.44541996717453003,\n",
       " 'eval_precision': 0.6823461091753774,\n",
       " 'eval_recall': 0.7669712793733682,\n",
       " 'eval_f1': 0.7221880762138906,\n",
       " 'eval_accuracy': 0.8824978012313105,\n",
       " 'eval_runtime': 14.6981,\n",
       " 'eval_samples_per_second': 28.439,\n",
       " 'eval_steps_per_second': 0.476,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 418\n",
      "  Batch size = 64\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:12<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AGR': {'precision': 0.8222222222222222,\n",
       "  'recall': 0.8980582524271845,\n",
       "  'f1': 0.8584686774941995,\n",
       "  'number': 206},\n",
       " 'AttributeValue': {'precision': 0.739413680781759,\n",
       "  'recall': 0.8284671532846716,\n",
       "  'f1': 0.7814113597246127,\n",
       "  'number': 274},\n",
       " 'CCI': {'precision': 0.9515418502202643,\n",
       "  'recall': 0.96,\n",
       "  'f1': 0.9557522123893806,\n",
       "  'number': 225},\n",
       " 'GBC': {'precision': 0.8849557522123894,\n",
       "  'recall': 0.8849557522123894,\n",
       "  'f1': 0.8849557522123894,\n",
       "  'number': 113},\n",
       " 'TBE': {'precision': 0.7671957671957672,\n",
       "  'recall': 0.8787878787878788,\n",
       "  'f1': 0.8192090395480227,\n",
       "  'number': 165},\n",
       " 'TEE': {'precision': 0.4271186440677966,\n",
       "  'recall': 0.4684014869888476,\n",
       "  'f1': 0.4468085106382979,\n",
       "  'number': 269},\n",
       " 'TSE': {'precision': 0.4808743169398907,\n",
       "  'recall': 0.6285714285714286,\n",
       "  'f1': 0.5448916408668729,\n",
       "  'number': 280},\n",
       " 'overall_precision': 0.6823461091753774,\n",
       " 'overall_recall': 0.7669712793733682,\n",
       " 'overall_f1': 0.7221880762138906,\n",
       " 'overall_accuracy': 0.8824978012313105}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [tags_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [tags_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to TimeClassification\n",
      "Configuration saved in TimeClassification/config.json\n",
      "Model weights saved in TimeClassification/pytorch_model.bin\n",
      "tokenizer config file saved in TimeClassification/tokenizer_config.json\n",
      "Special tokens file saved in TimeClassification/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"./TimeClassification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_working_phrases = [\n",
    "    \"Average delays caused by appealing to prefacture\",\n",
    "    \"Average time between fine creation and notification\",\n",
    "    \"Maximum time from fine creation to notification\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tell': 'O',\n",
       " 'me': 'O',\n",
       " 'the': 'O',\n",
       " 'average': 'AGR',\n",
       " 'time': 'O',\n",
       " 'from': 'O',\n",
       " 'opened': 'TSE',\n",
       " 'to': 'O',\n",
       " 'closed': 'TEE',\n",
       " 'more': 'CCI',\n",
       " 'than': 'CCI',\n",
       " 'seven': 'AttributeValue',\n",
       " 'days': 'O'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"Tell me the average time from opened to closed more than seven days\"\n",
    "tokens  = tokenizer(phrase.split(\" \"), return_tensors='pt', is_split_into_words=True, truncation=True)\n",
    "predictions = model(**tokens)\n",
    "logits = predictions[\"logits\"]\n",
    "predictions = logits.argmax(-1).tolist()[0]\n",
    "\n",
    "tags_list = ['B-TSE', 'B-TEE', 'I-AGR', 'I-TEE', 'I-AttributeValue', 'B-AGR', 'B-CCI', 'O', 'I-TBE', 'I-GBC', 'B-GBC', 'B-TBE', 'I-CCI', 'I-TSE', 'B-AttributeValue']\n",
    "\n",
    "ls = [tags_list[i] for i in predictions][1:-1]\n",
    "\n",
    "word_tag = {}\n",
    "tag_list_index = 0\n",
    "\n",
    "for word in phrase.split(\" \"):\n",
    "    tokenized_word = tokenizer(word, return_tensors='pt', add_special_tokens=False)\n",
    "    num_tokens = len(tokenized_word[\"input_ids\"][0])\n",
    "    regex =  re.search(r'^[BI]-(.*)',ls[tag_list_index])\n",
    "    if regex:\n",
    "        word_tag[word] = regex.group(1)\n",
    "    else:\n",
    "        word_tag[word] = ls[tag_list_index]\n",
    "    if num_tokens == 1:\n",
    "        tag_list_index += 1\n",
    "    else:\n",
    "        tag_list_index += num_tokens\n",
    "\n",
    "word_tag\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPIBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb8d1a381093ee6ff8b5e45ca9539dfe4d33cc67de644f90e8e7e8993859d491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
