{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javiervilarinomayo/opt/anaconda3/envs/ppi/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from firebase import firebase\n",
    "import json\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, load_metric\n",
    "import numpy as np\n",
    "import re\n",
    "import dotenv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura e integración de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos usado durante la realización de este trabajo es privado y no puede ser compartido, pero si se puede compartir el código de la lectura y la integración de los datos. El conjunto de datos final está formado por 3088 definiciones de indicadores de rendimiento de procesos (PPIs), donde cada indicador, además de incluir la definición, incluye una etiqueta asociada a cada palabra de la definición. \n",
    "\n",
    "El objetivo de este proyecto, es predecir la etiqueta asociada a cada palabra de la definición de un indicador de rendimiento de procesos (PPI). Esta etiquetación tiene una gran utilidad, ya que, si se realiza adecuadamente, se puede computar posteriormente el indicador de proceso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos iniciales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, realizamos la lectura del conjunto de datos inicial. Cabe destacar, que este conjunto de datos es bastante limitado, y que por tanto será necesario realizar una ampliación de los mismos a través de distintas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/parser_training_data.json', 'r') as f:\n",
    "    training_data = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos de crowdsourcing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera opción que planteamos para solventar el problema del tamaño inicial del conjunto de datos, fue la de realizar una ampliación de estos generando parafrases que posteriormente serían etiquedas por usuarios reales a través de una plataforma de crowdsourcing.\n",
    "\n",
    "El resultado de este proceso supuso una ampliación de los datos que resultó ser insuficiente para solventar el problema de tamaño del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "FIREBASE_URL = os.getenv(\"FIREBASE_URL\")    \n",
    "\n",
    "firebase_url = firebase.FirebaseApplication(FIREBASE_URL, None)\n",
    "\n",
    "best_paraphrases_firebase = firebase_url.get('/best_paraphrases', None)\n",
    "firebase_id = list(best_paraphrases_firebase)[0]\n",
    "best_paraphrases = best_paraphrases_firebase[firebase_id]\n",
    "\n",
    "paraphrases = firebase_url.get('/paraphrases', None)\n",
    "firebase_paraphrase_id = list(paraphrases)[0]\n",
    "tagged_paraphrases = paraphrases[firebase_paraphrase_id]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos generados con chatito"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de generar el mayor número de datos posible, se planteó la posibilidad de generar datos a través de la herramienta chatito. Esta herramienta permite realizar esta generación a partir de un fichero de definición de patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/training_chatito.json', 'r') as f:\n",
    "    training_chatito = json.load(f)[\"measures\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del conjunto de datos generado a través de la plataforma de crowdsourcing, es necesario realizar un preprocesamiento específico debido a la forma en la que los datos se almacenan en firebase y a las decisiones de diseño tomadas durante la implementación de la plataforma. \n",
    "\n",
    "En este caso diferenciamos entre parafrases etiquetadas que han sido confirmadas por un número relevante de usuarios y aquellas que no han sido totalmente confirmadas. Cabe destacar que debido a la escasa cantidad de datos, se ha decidido incluir todas las parafrases etiquetadas, independientemente de si han sido confirmadas o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_annotation(paraphrase, tagged_paraphrases):\n",
    "    same_paraphrases = [p[\"annotation\"] for p in tagged_paraphrases if p[\"description\"] == paraphrase]\n",
    "    number_occurrences = [same_paraphrases.count(a) for a in same_paraphrases]\n",
    "    max_occurrences = max(number_occurrences)\n",
    "    return same_paraphrases[number_occurrences.index(max_occurrences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrases = set([phrase[\"description\"] for phrase in tagged_paraphrases[\"data\"]])\n",
    "for paraphrase in paraphrases:\n",
    "    annotation = get_best_annotation(paraphrase, tagged_paraphrases[\"data\"])\n",
    "    best_paraphrases[\"data\"].append({\"description\": paraphrase, \"annotation\": annotation})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto al conjunto de datos generado a través de chatito, se realiza un preprocesamiento básico, donde se realiza una conversión de formato de los datos para que estén\n",
    " en el mismo formato que el conjunto de datos inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_chatito_parsed = []\n",
    "for phrase in training_chatito:\n",
    "    description = \"\".join([value[\"value\"] for value in phrase])\n",
    "    slots = [{'text': value[\"value\"].strip(), 'tag': 'O'} if \"slot\" not in value else {'text': value[\"value\"].strip(), 'tag': value[\"slot\"]} for value in phrase]\n",
    "    slots_cleaned = [slot for slot in slots if slot[\"text\"] != \"\"]\n",
    "    slots_list = []\n",
    "    for slot in slots_cleaned:\n",
    "        slots_list.append({'text': slot[\"text\"], 'tag': slot[\"tag\"]})\n",
    "    training_chatito_parsed.append({\"description\": description, \"annotation\": slots_list})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integración de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez están los datos preparados, se procede a su integración. En este caso, se realiza una integración de los datos de forma que se mantengan los datos originales y se añadan los nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_paraphrases[\"data\"].extend(training_data[\"data\"])\n",
    "best_paraphrases[\"data\"].extend(training_chatito_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases: 3190\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of phrases: {len(best_paraphrases[\"data\"])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, decidimos centrar el alcance del proyecto a PPIs relacionados con métricas temporales. Para ello, se realiza un filtrado de los datos para quedarnos únicamente con aquellos PPIs que contienen etiquetas relacionadas con métricas temporales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_TAGS = [\"TMI\", \"TSI\", \"TSE\", \"TEI\", \"TEE\", \"TBE\"]\n",
    "COUNT_TAGS = [\"CMI\", \"CE\"]\n",
    "DATA_TAGS = [\"AttributeName, AttributeValue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time phrases: 3088\n",
      "Number of count phrases: 81\n",
      "Number of data phrases: 0\n"
     ]
    }
   ],
   "source": [
    "time_phrases = []\n",
    "count_phrases = []\n",
    "data_phrases = []\n",
    "\n",
    "for phrase in best_paraphrases[\"data\"]:\n",
    "    text = phrase[\"description\"]\n",
    "    labels = set([label[\"tag\"] for label in phrase[\"annotation\"]])\n",
    "    if len(labels.intersection(TIME_TAGS)) > 0:\n",
    "        time_phrases.append(phrase)\n",
    "    elif len(labels.intersection(COUNT_TAGS)) > 0:\n",
    "        count_phrases.append(phrase)\n",
    "    elif len(labels.intersection(DATA_TAGS)) > 0:\n",
    "        data_phrases.append(phrase)\n",
    "\n",
    "print(f'Number of time phrases: {len(time_phrases)}')\n",
    "print(f'Number of count phrases: {len(count_phrases)}')\n",
    "print(f'Number of data phrases: {len(data_phrases)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reajuste del formato"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes pasos, se formatean los datos para que se ajusten a los requerimientos de la libreria datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "useless_tags = [\"TMI\", \"TSI\", \"TEI\", \"GBI\"]\n",
    "\n",
    "for phrase in time_phrases:\n",
    "    annotations = []\n",
    "    for slot in phrase[\"annotation\"]:\n",
    "        slot_object = {}\n",
    "        slot_object[\"value\"] = slot[\"text\"]\n",
    "        slot_object[\"type\"] = \"Slot\"\n",
    "        slot_object[\"slot\"] = slot[\"tag\"] if slot[\"tag\"] not in useless_tags else \"O\"\n",
    "        annotations.append(slot_object)\n",
    "\n",
    "    data.append(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tags = []\n",
    "\n",
    "for phrase in data:\n",
    "    phrase_tokens = []\n",
    "    phrase_tags = []\n",
    "    for slot in phrase:\n",
    "        splits = slot[\"value\"].split(\" \")\n",
    "        tag = slot[\"slot\"]\n",
    "        for i in range(len(splits)):\n",
    "            if tag != \"O\":\n",
    "                if i == 0:\n",
    "                    phrase_tokens.append(splits[i])\n",
    "                    phrase_tags.append(\"B-\"+tag)\n",
    "                else:\n",
    "                    phrase_tokens.append(splits[i])\n",
    "                    phrase_tags.append(\"I-\"+tag)\n",
    "            else:\n",
    "                phrase_tokens.append(splits[i])\n",
    "                phrase_tags.append(tag)\n",
    "    tokens.append(phrase_tokens)\n",
    "    tags.append(phrase_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez procesados los datos, se procede a la instanciación de la clase Dataset. Además, generamos la lista de etiquetas codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-TBE', 'I-AttributeValue', 'I-GBC', 'I-AGR', 'B-TEE', 'B-CCI', 'B-AGR', 'I-TSE', 'I-TEE', 'B-GBC', 'B-AttributeValue', 'B-TSE', 'I-TBE', 'O', 'I-CCI']\n"
     ]
    }
   ],
   "source": [
    "tags_list = list(set([tag for phrase in tags for tag in phrase]))\n",
    "print(tags_list)\n",
    "\n",
    "labels = [[tags_list.index(label) for label in phrase] for phrase in tags]\n",
    "\n",
    "examples = {\n",
    "    \"tokens\": tokens,\n",
    "    \"tags\": labels\n",
    "}\n",
    "\n",
    "datasets = Dataset.from_dict(examples).train_test_split(test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el preprocesamiento de los datos, antes de realizar el fine-tuning del modelo, es necesario realizar un tokenizado de los datos. Para ello, se utiliza la clase AutoTokenizer de la librería transformers. Cabe destacar, que se usa el tokenizador de bert-base-uncased, ya que es el modelo que se usará para realizar el fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.00ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 25.48ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tokenizados los datos, podemos proceder a preparar el fine-tuning del modelo. Para ello, se utiliza la clase BertForTokenClassification de la librería transformers para cargar el modelo preentrenado. En este punto, configuramos ciertos parámetros del modelo, como el número de epochs, el tamaño del batch, el tamaño del learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/javiervilarinomayo/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"TimeClassification\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos DataCollatorForTokenClassification, que realizará el padding dinámico de los datos, para que todos los datos tengan el mismo tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, cargamos la métrica seqeval que nos permitirá evaluar el modelo a través de distintas métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [tags_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [tags_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez está todo listo, procedemos a realizar el fine-tuning del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2470\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 117\n",
      "  Number of trainable parameters = 108903183\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 39/117 [04:08<07:07,  5.48s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 618\n",
      "  Batch size = 64\n",
      "                                                \n",
      " 33%|███▎      | 39/117 [04:29<07:07,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6913461089134216, 'eval_precision': 0.5617005267118134, 'eval_recall': 0.6528202885876694, 'eval_f1': 0.6038422649140545, 'eval_accuracy': 0.8204946160217725, 'eval_runtime': 21.1813, 'eval_samples_per_second': 29.177, 'eval_steps_per_second': 0.472, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 78/117 [08:38<03:41,  5.67s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 618\n",
      "  Batch size = 64\n",
      "                                                \n",
      " 67%|██████▋   | 78/117 [08:59<03:41,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1956333965063095, 'eval_precision': 0.8754654530409599, 'eval_recall': 0.9252295583734149, 'eval_f1': 0.8996598639455782, 'eval_accuracy': 0.9604780499349189, 'eval_runtime': 21.4215, 'eval_samples_per_second': 28.85, 'eval_steps_per_second': 0.467, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [13:56<00:00,  7.45s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 618\n",
      "  Batch size = 64\n",
      "                                                 \n",
      "100%|██████████| 117/117 [14:18<00:00,  7.45s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 117/117 [14:18<00:00,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11802513152360916, 'eval_precision': 0.9482242190842961, 'eval_recall': 0.9689549628334062, 'eval_f1': 0.9584775086505191, 'eval_accuracy': 0.9814223168855757, 'eval_runtime': 21.2059, 'eval_samples_per_second': 29.143, 'eval_steps_per_second': 0.472, 'epoch': 3.0}\n",
      "{'train_runtime': 858.1347, 'train_samples_per_second': 8.635, 'train_steps_per_second': 0.136, 'train_loss': 0.7391044421073718, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [14:19<00:00,  7.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=117, training_loss=0.7391044421073718, metrics={'train_runtime': 858.1347, 'train_samples_per_second': 8.635, 'train_steps_per_second': 0.136, 'train_loss': 0.7391044421073718, 'epoch': 3.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el siguiente código, podemos visualizar el resultado del fine-tuning, obteniendo el accuracy, el f1-score, recall y precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 618\n",
      "  Batch size = 64\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11802513152360916,\n",
       " 'eval_precision': 0.9482242190842961,\n",
       " 'eval_recall': 0.9689549628334062,\n",
       " 'eval_f1': 0.9584775086505191,\n",
       " 'eval_accuracy': 0.9814223168855757,\n",
       " 'eval_runtime': 22.6685,\n",
       " 'eval_samples_per_second': 27.263,\n",
       " 'eval_steps_per_second': 0.441,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, una vez entrenado y evaluado el modelo, podemos guardar el modelo para poder utilizarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to TimeClassification\n",
      "Configuration saved in TimeClassification/config.json\n",
      "Model weights saved in TimeClassification/pytorch_model.bin\n",
      "tokenizer config file saved in TimeClassification/tokenizer_config.json\n",
      "Special tokens file saved in TimeClassification/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar predicciones, debemos cargar el modelo que hemos entrenado anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./TimeClassification/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./TimeClassification/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at ./TimeClassification.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"./TimeClassification\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código, permite realizar predicciones sobre una definición de PPI. Para ello, se tokeniza la entrada, se calcula la predicción sobre la misma y se realiza un postprocesado para poder mostrar correctamente el resultado. Cabe destacar que en el postprocesado, es necesario tener en cuenta que cada palabra puede estar compuesta por varias palabras tokenizadas. En este caso, decidimos que la etiqueta de la palabra compuesta sería la etiqueta de la primera palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': 'AGR',\n",
       " 'time': 'O',\n",
       " 'to': 'TBE',\n",
       " 'resolve': 'TBE',\n",
       " 'an': 'TBE',\n",
       " 'incident': 'TBE',\n",
       " 'grouped': 'O',\n",
       " 'by': 'O',\n",
       " 'impact': 'GBC'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"average time to resolve an incident grouped by impact\"\n",
    "tokens  = tokenizer(phrase.split(\" \"), return_tensors='pt', is_split_into_words=True, truncation=True)\n",
    "predictions = model(**tokens)\n",
    "logits = predictions[\"logits\"]\n",
    "predictions = logits.argmax(-1).tolist()[0]\n",
    "\n",
    "ls = [tags_list[i] for i in predictions][1:-1]\n",
    "\n",
    "word_tag = {}\n",
    "tag_list_index = 0\n",
    "\n",
    "for word in phrase.split(\" \"):\n",
    "    tokenized_word = tokenizer(word, return_tensors='pt', add_special_tokens=False)\n",
    "    num_tokens = len(tokenized_word[\"input_ids\"][0])\n",
    "    regex =  re.search(r'^[BI]-(.*)',ls[tag_list_index])\n",
    "    if regex:\n",
    "        word_tag[word] = regex.group(1)\n",
    "    else:\n",
    "        word_tag[word] = ls[tag_list_index]\n",
    "    if num_tokens == 1:\n",
    "        tag_list_index += 1\n",
    "    else:\n",
    "        tag_list_index += num_tokens\n",
    "\n",
    "word_tag\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a440e77113f1dc537e3c2200ff0bea505e3a8531e0aba18f9b69ee976ca00316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
